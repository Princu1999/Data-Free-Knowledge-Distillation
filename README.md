
# Adversarial Knowledge Distillation (AKD) with GANs â€” CIFAR-100

> Modular PyTorch refactor of your Colab/monolithic script into a clean, GitHubâ€‘ready repo. > Implements **dataâ€‘free adversarial knowledge distillation** (ResNetâ€‘34 teacher â†’ compact students) > and a tiny generator to synthesize training stimuli.

<p align="center">
<a href="#-quickstart">Quickstart</a> â€¢
<a href="#-project-structure">Structure</a> â€¢
<a href="#-results">Results</a> â€¢
<a href="#-training--evaluation">Training</a> â€¢
<a href="#-references">References</a>
</p>

---

## Highlights
- **Teacher**: ResNetâ€‘34 (pretrained on CIFARâ€‘100) distills logits to students without real training data.  
- **Students**: (i) `ResNet18_8x_Small` (~20% teacher params), (ii) `ResNet18_8x` (~50% teacher params).  
- **GeneratorA**: Lightweight GANâ€‘style image sampler (latent `zâ†’32Ã—32`) to drive KD.

## Quickstart
```bash
# 1) Install
pip install -r requirements.txt

# 2) (Optional) Place teacher weights
#    ./teacher/best_resnet34_cifar100.pth

# 3) Train + evaluate (writes run folders in ./logs, ./models, ./results)
python scripts/train_akd.py
```

To sample images from the generator:
```bash
python scripts/generate_images.py
```

## ğŸ—‚ Project Structure
```
akd_kd_gan/
  akd_kd_gan/
    __init__.py
    config.py                 # AKDConfig: paths, hparams, seeds, run folders
    data.py                   # CIFARâ€‘100 loader for testâ€‘only split
    losses.py                 # KLâ€‘KD, diversity loss (utility)
    models/
      generator.py            # GeneratorA (zâ†’32Ã—32)
      resnet_small.py         # BasicBlock/Bottleneck/ResNet/ResNet18_8x/_Small
    engine/
      train.py                # train_epoch() (alternate student/gen updates)
      eval.py                 # evaluate() â†’ CE loss & accuracy
    utils/
      logger.py, early_stopping.py, checkpoint.py
  scripts/
    train_akd.py              # endâ€‘toâ€‘end AKD training loop (teacherâ†’students)
    generate_images.py        # grid + individual PNGs
requirements.txt
README.md
```

## Results
Key numbers extracted from your report:

- **Accuracy (20% test split)** â€” Studentâ€‘50%: **39.9%**, Studentâ€‘10%: **20%**   (CIFARâ€‘100, teacher = ResNetâ€‘34). îˆ€fileciteîˆ‚turn1file1îˆ‚L127-L135îˆ  
- **Accuracy (10% test split)** â€” Studentâ€‘50%: **38.10%**, Studentâ€‘10%: **19.01%**. îˆ€fileciteîˆ‚turn1file1îˆ‚L136-L141îˆ  
- **Parameter counts** â€” Teacher: **21,335,972**; Studentâ€‘10%: **2,820,740**; Studentâ€‘50%: **11,220,132**. îˆ€fileciteîˆ‚turn1file9îˆ‚L47-L52îˆ

> The codebase mirrors your original training procedure (alternate student and generator updates and evaluation on CIFARâ€‘100 test subsets). îˆ€fileciteîˆ‚turn1file3îˆ‚L36-L63îˆ îˆ€fileciteîˆ‚turn1file2îˆ‚L41-L45îˆ

<details>
<summary><b>Method summary (from your report)</b></summary>

- Distill teacher â†’ students via KL on synthetic images from the generator; update generator adversarially to maximize studentâ€‘teacher gap. îˆ€fileciteîˆ‚turn1file1îˆ‚L56-L61îˆ  
- Dataset is **testâ€‘only CIFARâ€‘100**; original training data is not used. îˆ€fileciteîˆ‚turn1file1îˆ‚L19-L22îˆ
</details>

## Training & Evaluation
- Configure defaults in `akd_kd_gan/config.py` (paths, learning rates, epochs, etc.).  
- `scripts/train_akd.py` will:
  1) build teacher (loads `./teacher/best_resnet34_cifar100.pth` if present),  
  2) train **Studentâ€‘50%** and **Studentâ€‘20%** alternating with the generator,  
  3) evaluate after each epoch and save best checkpoints to `./models/<run_id>/`.

> The refactor keeps the original generator and student update schedule (15Ã— student steps, then 1Ã— generator step) for parity with the notebook. îˆ€fileciteîˆ‚turn1file3îˆ‚L48-L56îˆ

## Notes
- The small student architecture retains the slimmed channels you used (~20% teacher params). îˆ€fileciteîˆ‚turn1file5îˆ‚L31-L46îˆ  
- Run metadata & `config.json` are automatically written under `./logs/<run_id>/` (reproducibility & audit). îˆ€fileciteîˆ‚turn1file6îˆ‚L39-L50îˆ

## References
Your report lists the core AKD/KD papers (Hinton et al., Micaelli & Storkey, etc.). îˆ€fileciteîˆ‚turn1file8îˆ‚L30-L48îˆ

---

> This repository was generated by refactoring your uploaded script while preserving behavior (training loop, models, and generator) and surfacing the most important results for quick understanding.
